{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer #ai that can be used to train on parts of speech\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "path =\"metadata.csv\" \n",
    "df = pd.read_csv(path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories = [\"biorxiv_medrxiv\",\"comm_use_subset\",\"custom_license\",\"noncomm_use_subset\"]\n",
    "directories = [\"biorxiv_medrxiv\"]\n",
    "json_array = []\n",
    "for d in directories:\n",
    "    for file in os.listdir(f\"{d}/{d}\"):\n",
    "        #print(file)\n",
    "        file_path = f\"{d}/{d}/{file}\"\n",
    "        j = json.load(open(file_path, \"rb\"))\n",
    "        json_array.append(j)\n",
    "#finds json files and saves to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id\n",
      "metadata\n",
      "abstract\n",
      "body_text\n",
      "bib_entries\n",
      "ref_entries\n",
      "back_matter\n"
     ]
    }
   ],
   "source": [
    "#search_keys \n",
    "for key in json_array[0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with 0 out of 885 lol I need to automate this but good for first test run\n",
    "raw_abstract = []\n",
    "raw_metadata = []\n",
    "raw_bodytext = []\n",
    "raw_bib_entries = []\n",
    "raw_ref = []\n",
    "raw_back_mater = []\n",
    "for i in range(len(json_array)):\n",
    "    raw_abstract.append(json_array[i]['abstract'])\n",
    "    raw_metadata .append(json_array[i]['metadata'])\n",
    "    raw_bodytext.append(json_array[i]['body_text'])\n",
    "    raw_bib_entries.append(json_array[i]['bib_entries'])\n",
    "    raw_ref.append(json_array[i]['ref_entries'])\n",
    "    raw_back_mater.append(json_array[i]['back_matter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_full_text = []\n",
    "for i in range(len(raw_bodytext)):\n",
    "    for key in raw_bodytext[i]:\n",
    "        body_full_text.append(key['text'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------##\n",
    "#raw_bodytext\n",
    "#body_full_text = []\n",
    "#for key in raw_bodytext:\n",
    "    #body_full_text.append(key['text'] + '\\n\\n')\n",
    "#    print(key['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30181"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_full_text)#there are 20 paragraphs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs more structuring.\n",
    "#Tasks as per Kaggle:- what is known about transmission/incubation and environmental stability?\n",
    "###Points to consider::--- Extract meaning(NLP?) Terms displayed in Tasks may be called something else in the dataset/research papers...(WordNet)\n",
    "#body_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range of incubation periods for the disease in humans (and how this varies across age and health status)\n",
    "#and how long individuals are contagious, even after recovery.\n",
    "#body_full_text[2]\n",
    "#tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for i in range(len(body_full_text)):\n",
    "    paragraphs.append(sent_tokenize(body_full_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs[0]) #how many sentences are in a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30181"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first step, remove stop words\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(paragraphs):\n",
    "    temp = len(paragraphs[i])\n",
    "    j = 0\n",
    "    while j < temp:\n",
    "        temp_word = paragraphs[i][j]\n",
    "        temp_array = []\n",
    "        temp_filtered = []\n",
    "        temp_array = word_tokenize(temp_word)\n",
    "        for word in temp_array:\n",
    "            if word not in stop_words:\n",
    "                temp_filtered.append(word)\n",
    "        paragraphs[i][j] = temp_filtered\n",
    "        j = j+ 1\n",
    "    i = i+ 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragraphs[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for range in incubation periods in humans only and how it varies across age and health status.\n",
    "Step 1, generate a \"similarity index\" for each paragraph. The higher the similarity index, the more likely we are to find what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#target_words = [\"incubation\",\"humans\",\"age\",\"health\"]\n",
    "target_words = [\"incubation\",\"humans\"]\n",
    "target_length = len(target_words)\n",
    "print (target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet will be used to acquire these similarities. For my dictionary, i only need associated paragraph/sentence and attatched overall similarity in the form of a percentage. Start with base similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#percentage\n",
    "sentense_score_array = []\n",
    "i = 0\n",
    "while i < len(paragraphs):\n",
    "    temp = len(paragraphs[i])\n",
    "    j = 0\n",
    "    while j < temp:\n",
    "        temp_word = paragraphs[i][j]\n",
    "        k = 0\n",
    "        temp_score = 0\n",
    "        raw_word_score = 0\n",
    "        for word in temp_word:\n",
    "            while k < target_length:\n",
    "                try:\n",
    "                    w1=wordnet.synsets(word)[0]\n",
    "                    w2=wordnet.synsets(target_words[k])[0]\n",
    "                    percentage = (w1.wup_similarity(w2))*100\n",
    "                except:\n",
    "                    percentage = 0\n",
    "                percentage+=percentage\n",
    "                k = k + 1\n",
    "            raw_word_score += (percentage/target_length)\n",
    "        try:\n",
    "            raw_sentense_score = raw_word_score/len(temp_word)\n",
    "        except:\n",
    "            raw_sentense_score = 0\n",
    "        #print(\"sentense_score: \",raw_sentense_score)\n",
    "        sentense_score_array.append(raw_sentense_score)\n",
    "        j = j+ 1\n",
    "    i = i+ 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentense_score_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.104143245669704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.55172413793109"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(sentense_score_array))\n",
    "max(sentense_score_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 22), match='523 days'>\n",
      "<re.Match object; span=(27, 35), match='12 weeks'>\n",
      "at\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#remember..should be raw strings!\n",
    "x = \"incubation is 523 days and 12 weeks\"\n",
    "pattern = re.compile(r'\\d+ (days|day|weeks|week|months|month|years|year)')\n",
    "matches = pattern.finditer(x)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    \n",
    "print(x[5:7]) #string slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

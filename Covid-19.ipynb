{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer #ai that can be used to train on parts of speech\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "path =\"metadata.csv\" \n",
    "df = pd.read_csv(path, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directories = [\"biorxiv_medrxiv\",\"comm_use_subset\",\"custom_license\",\"noncomm_use_subset\"]\n",
    "directories = [\"biorxiv_medrxiv\"]\n",
    "json_array = []\n",
    "for d in directories:\n",
    "    for file in os.listdir(f\"{d}/{d}\"):\n",
    "        #print(file)\n",
    "        file_path = f\"{d}/{d}/{file}\"\n",
    "        j = json.load(open(file_path, \"rb\"))\n",
    "        json_array.append(j)\n",
    "#finds json files and saves to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id\n",
      "metadata\n",
      "abstract\n",
      "body_text\n",
      "bib_entries\n",
      "ref_entries\n",
      "back_matter\n"
     ]
    }
   ],
   "source": [
    "#search_keys \n",
    "for key in json_array[0]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with 0 out of 885 lol I need to automate this but good for first test run\n",
    "raw_abstract = []\n",
    "raw_metadata = []\n",
    "raw_bodytext = []\n",
    "raw_bib_entries = []\n",
    "raw_ref = []\n",
    "raw_back_mater = []\n",
    "for i in range(len(json_array)):\n",
    "    raw_abstract.append(json_array[i]['abstract'])\n",
    "    raw_metadata .append(json_array[i]['metadata'])\n",
    "    raw_bodytext.append(json_array[i]['body_text'])\n",
    "    raw_bib_entries.append(json_array[i]['bib_entries'])\n",
    "    raw_ref.append(json_array[i]['ref_entries'])\n",
    "    raw_back_mater.append(json_array[i]['back_matter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_full_text = []\n",
    "for i in range(len(raw_bodytext)):\n",
    "    for key in raw_bodytext[i]:\n",
    "        body_full_text.append(key['text'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##------------------------------------------------------------##\n",
    "#raw_bodytext\n",
    "#body_full_text = []\n",
    "#for key in raw_bodytext:\n",
    "    #body_full_text.append(key['text'] + '\\n\\n')\n",
    "#    print(key['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30181"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body_full_text)#there are 20 paragraphs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs more structuring.\n",
    "#Tasks as per Kaggle:- what is known about transmission/incubation and environmental stability?\n",
    "###Points to consider::--- Extract meaning(NLP?) Terms displayed in Tasks may be called something else in the dataset/research papers...(WordNet)\n",
    "#body_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Range of incubation periods for the disease in humans (and how this varies across age and health status)\n",
    "#and how long individuals are contagious, even after recovery.\n",
    "#body_full_text[2]\n",
    "#tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for i in range(len(body_full_text)):\n",
    "    paragraphs.append(sent_tokenize(body_full_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs[0]) #how many sentences are in a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30181"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first step, remove stop words\n",
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to', 'no', 'being', 'yourself', 'are', 'because', 'on', 'was', 'you', 'will', 'our', 'few', 'further', 'can', 'i', 'were', 'ma', \"it's\", 'at', 'am', 'each', 'hadn', \"hadn't\", 'them', 'yours', 'out', 'do', 'couldn', 'which', 'she', 'if', 'hers', 'nor', 'did', 'this', 'into', 've', 'from', 'isn', 'not', 'been', 'with', 'haven', 'ain', 'between', 'd', \"haven't\", 'same', 'himself', 'before', 'who', 'does', 'where', 'll', 'its', 'by', 'him', 'her', 'needn', 'that', 'an', 'once', \"weren't\", 'won', 'these', 'and', 'whom', 'those', 'until', 're', 'only', \"won't\", 'yourselves', 'is', \"didn't\", 'has', 'of', 'didn', 'off', \"wasn't\", \"mightn't\", \"needn't\", 'mightn', 'or', 'but', 'your', 'y', 'below', 'more', 'own', 'how', 'doing', 'under', 'through', 't', 'above', 'a', 'having', 'the', \"aren't\", \"isn't\", \"doesn't\", \"don't\", \"mustn't\", 'shan', 'again', 'over', \"hasn't\", \"you've\", 'after', 'up', 'we', 's', 'don', 'some', 'his', 'both', 'their', 'all', 'o', \"couldn't\", \"you're\", \"you'd\", 'wasn', 'had', 'very', 'myself', 'against', 'shouldn', 'ourselves', 'weren', \"she's\", 'it', 'doesn', 'such', 'be', \"shan't\", \"wouldn't\", 'itself', 'herself', \"you'll\", 'during', 'he', 'there', 'm', 'other', 'what', 'ours', 'my', 'wouldn', 'then', 'down', 'have', 'just', 'theirs', \"should've\", 'here', 'about', 'as', 'any', \"shouldn't\", 'me', 'should', 'while', 'most', 'too', 'now', 'aren', 'so', 'why', \"that'll\", 'in', 'hasn', 'when', 'themselves', 'mustn', 'than', 'for', 'they'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(paragraphs):\n",
    "    temp = len(paragraphs[i])\n",
    "    j = 0\n",
    "    while j < temp:\n",
    "        temp_word = paragraphs[i][j]\n",
    "        temp_array = []\n",
    "        temp_filtered = []\n",
    "        temp_array = word_tokenize(temp_word)\n",
    "        for word in temp_array:\n",
    "            if word not in stop_words:\n",
    "                temp_filtered.append(word)\n",
    "        paragraphs[i][j] = temp_filtered\n",
    "        j = j+ 1\n",
    "    i = i+ 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paragraphs[1][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for range in incubation periods in humans only and how it varies across age and health status.\n",
    "Step 1, generate a \"similarity index\" for each paragraph. The higher the similarity index, the more likely we are to find what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "target_words = [\"incubation\",\"humans\",\"age\",\"health\"]\n",
    "#target_words = [\"incubation\",\"humans\"]\n",
    "target_length = len(target_words)\n",
    "print (target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30181"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet will be used to acquire these similarities. For my dictionary, i only need associated paragraph/sentence and attatched overall similarity in the form of a percentage. Start with base similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of filtered sentences 0\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "#percentage\n",
    "sentense_score_array = []\n",
    "filtered_sentences = []\n",
    "i = 0\n",
    "while i < len(paragraphs):\n",
    "    temp = len(paragraphs[i])\n",
    "    j = 0\n",
    "    while j < temp:\n",
    "        temp_word = paragraphs[i][j]\n",
    "        k = 0\n",
    "        temp_score = 0\n",
    "        raw_word_score = 0\n",
    "        for word in temp_word:\n",
    "            while k < target_length:\n",
    "                try:\n",
    "                    w1=wordnet.synsets(word)[0]\n",
    "                    w2=wordnet.synsets(target_words[k])[0]\n",
    "                    percentage = (w1.wup_similarity(w2))*100\n",
    "                except:\n",
    "                    percentage = 0\n",
    "                percentage+=percentage\n",
    "                k = k + 1\n",
    "            raw_word_score += (percentage/target_length)\n",
    "        try:\n",
    "            raw_sentense_score = raw_word_score/len(temp_word)\n",
    "        except:\n",
    "            raw_sentense_score = 0\n",
    "        #print(\"sentense_score: \",raw_sentense_score)\n",
    "        sentense_score_array.append(raw_sentense_score)\n",
    "        if(raw_sentense_score > 20):\n",
    "            filtered_sentences.append(paragraphs[i][j])    \n",
    "        j = j+ 1\n",
    "    i = i+ 1  \n",
    "print(\"Amount of filtered sentences\", len(filtered_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-c95fffbd2f24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#sentense_score_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfiltered_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#sentense_score_array\n",
    "filtered_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate senteces for pattern recognition\n",
    "i = 0\n",
    "temp = \"\"\n",
    "while i < len(filtered_sentences[0]):\n",
    "    temp+=(filtered_sentences[0][i] + \" \")\n",
    "    i+=1\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences[0] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate for pattern matching\n",
    "k = 0\n",
    "while k < len(filtered_sentences):\n",
    "    i = 0\n",
    "    temp = \"\"\n",
    "    while i < len(filtered_sentences[k]):\n",
    "        temp+=(filtered_sentences[k][i] + \" \")\n",
    "        i+=1\n",
    "    filtered_sentences[k] = temp\n",
    "    #print(temp)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "print(statistics.mean(sentense_score_array))\n",
    "max(sentense_score_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#remember..should be raw strings!\n",
    "#x = \"incubation is 523 days and 12 weeks\"\n",
    "#x = filtered_sentences[0]\n",
    "#pattern = re.compile(r'\\d+ (days|day|weeks|week|months|month|years|year)')\n",
    "#matches = pattern.finditer(x)\n",
    "#match_list = []\n",
    "#for match in matches:\n",
    "#    match_list.append(match)\n",
    "#    print(match)\n",
    "    \n",
    "#print(x[5:7]) #string slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember..should be raw strings!\n",
    "import re\n",
    "i = 0\n",
    "while i < len(filtered_sentences):\n",
    "    pattern = re.compile(r'\\d+ (days|day|weeks|week|months|month|years|year)')\n",
    "    matches = pattern.finditer(filtered_sentences[i])\n",
    "    match_list = []\n",
    "    for match in matches:\n",
    "        match_list.append(match)\n",
    "        print(match)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-77e60338b0b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmatch_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "match_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
